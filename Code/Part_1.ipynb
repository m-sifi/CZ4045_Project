{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Sequence Tagging: NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 12:49:57.977742: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-28 12:49:57.977781: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-28 12:49:57.978729: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-28 12:49:58.057783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-28 12:49:58.835992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader\n",
    "import numpy as np\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional\n",
    "import keras as k\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qn 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"student\", \"Apple\", \"apple\"]\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print(\"Word\\t\\tMost similar word\\tCosine similarity\")\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "for word in words:\n",
    "    most_similar = w2v.most_similar(positive=[word])\n",
    "    print(f\"{word}\\t\\t{most_similar[0][0]}  \\t\\t{most_similar[0][1]}\")\n",
    "print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoNLL2003_dir = '../Datasets/CoNLL2003_dataset'\n",
    "train_dir = f'{CoNLL2003_dir}/eng.train'\n",
    "dev_dir =  f'{CoNLL2003_dir}/eng.testa'\n",
    "test_dir =  f'{CoNLL2003_dir}/eng.testb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_content(path):\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            content = file.readlines()\n",
    "        file.close()\n",
    "    except Exception as e:\n",
    "        content = None\n",
    "        print(e)\n",
    "    \n",
    "    return content\n",
    "\n",
    "def print_items(item):\n",
    "    for s in item: print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content = import_content(train_dir)\n",
    "dev_content = import_content(dev_dir)\n",
    "test_content = import_content(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(content):\n",
    "    split_data = [c.split(' ') for c in content] if content != None else []\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    words = []\n",
    "\n",
    "    for line in split_data:\n",
    "        # if end of a sentence\n",
    "        if line == ['\\n']:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            s_text  = line[0]\n",
    "            s_tag = line[-1].replace('\\n','')\n",
    "\n",
    "            sentence.append([s_text, s_tag]) \n",
    "            words.append([s_text, s_tag])\n",
    "    \n",
    "    sentences.append(sentence) # last item in content not new line so must add previous sentence manually after loop           \n",
    "\n",
    "    return sentences, words\n",
    "\n",
    "def split_text_tag(sentences):\n",
    "    text = []\n",
    "    tag = []\n",
    "    combined = []\n",
    "    sentence_count = 1\n",
    "\n",
    "    for s in sentences:\n",
    "        for w in s:\n",
    "            w_text  = w[0]\n",
    "            w_tag = w[-1].replace('\\n','')\n",
    "\n",
    "            text.append(w_text)\n",
    "            tag.append(w_tag)        \n",
    "            combined.append({\n",
    "                'sentence': sentence_count,\n",
    "                'text' : w_text,\n",
    "                'tag' : w_tag\n",
    "            })   \n",
    "        sentence_count+=1       \n",
    "    return text, tag, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_words = split_sentences(train_content)\n",
    "dev_sentences, dev_words = split_sentences(dev_content)\n",
    "test_sentences, test_words = split_sentences(test_content)\n",
    "\n",
    "train_text, train_tag, train_combined = split_text_tag(train_sentences)\n",
    "dev_text, dev_tag, dev_combined = split_text_tag(dev_sentences)\n",
    "test_text, test_tag, test_combined = split_text_tag(test_sentences)\n",
    "\n",
    "train_df = pd.DataFrame(train_combined)\n",
    "dev_df = pd.DataFrame(dev_combined)\n",
    "test_df = pd.DataFrame(test_combined)\n",
    "\n",
    "max_sentence_len = max([len(s) for s in train_sentences])\n",
    "\n",
    "voc = list(set(train_text))\n",
    "voc = np.append(voc,'<UNK>')\n",
    "voc = np.append(voc,'<PAD>')\n",
    "n_voc = len(voc)\n",
    "word2idx = {k: v for v, k in enumerate(voc)}\n",
    "\n",
    "tag_set = list(set(train_tag))\n",
    "n_tags = len(tag_set)\n",
    "tag2idx = {k: v for v, k in enumerate(tag_set)}\n",
    "\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qn 1.2 (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the size (number of sentences) of the training, development and test file for CoNLL2003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of sentences (training):\", len(train_sentences))\n",
    "print(\"Number of sentences (dev):\", len(dev_sentences))\n",
    "print(\"Number of sentences (test):\", len(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO, etc.) you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tag set (BIO):\", tag_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qn 1.2 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose an example sentence from the training set of CoNLL2003 that has at least two named entities with more than one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_ne_sentence(sentences):\n",
    "    for sentence in sentences:\n",
    "        ne_count = 0\n",
    "        for word_info in sentence:\n",
    "            if \"B-\" in word_info[-1]:\n",
    "                ne_count+=1\n",
    "        if ne_count == 2:\n",
    "            return sentence\n",
    "    return None        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = get_multiple_ne_sentence(train_sentences)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain how to form complete named entities from the label for each word, and list all the named entities in this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_entities(sentence):\n",
    "    inside_tags = ['I-ORG', 'I-LOC', 'I-PER', 'I-MISC'] # Tags that require multiple words to form an entity\n",
    "    begin_tags = ['B-LOC', 'B-ORG', 'B-MISC'] # Tags that are single word entities\n",
    "    outside_tags = ['O']\n",
    "    entities = [] # all entities gotten from search\n",
    "    entity = [] # word group of current entity if any group tags encountered\n",
    "    \n",
    "    for c in sentence:\n",
    "        if (c['tag'] in begin_tags or c['tag'] in outside_tags or c['tag'] == '\\n') and len(entity) != 0:\n",
    "            entities.append(' '.join(entity))\n",
    "            entity = []\n",
    "        if c['tag'] in begin_tags or c['tag'] in inside_tags: \n",
    "            entity.append(c['text'])\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,sentence_text_tag = split_text_tag([sentence])\n",
    "print(\"Complete named entities in the sentence:\", get_named_entities(sentence_text_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = w2v.vectors\n",
    "google_vocab_size, embedding_size = pretrained_weights.shape\n",
    "\n",
    "w2v_word2idx = dict(w2v.key_to_index)\n",
    "w2v_voc = w2v.index_to_key\n",
    "w2v_word2idx['<UNK>'] = w2v_word2idx[list(w2v_word2idx.keys())[-1]]+1\n",
    "w2v_word2idx['<PAD>'] = w2v_word2idx[list(w2v_word2idx.keys())[-1]]+1\n",
    "\n",
    "print(\"Embedding Size: \", embedding_size)\n",
    "print(\"Max Sentence Len: \", max_sentence_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for v in w2v_voc:\n",
    "  embeddings_index[v] =  w2v[v] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((n_voc, embedding_size))\n",
    "for word, i in word2idx.items():    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector[0:embedding_size]\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_sequence(sentences):\n",
    "  sequence = []\n",
    "  sent_seq = []\n",
    "  for s in sentences:\n",
    "    for w in s:\n",
    "      if w[0] in word2idx.keys():\n",
    "        sent_seq.append(word2idx[w[0]])\n",
    "      else:\n",
    "        sent_seq.append(word2idx['<UNK>'])\n",
    "    sequence.append(sent_seq)\n",
    "    sent_seq = []\n",
    "  \n",
    "  return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_x_sequence(train_sentences)\n",
    "x_train = pad_sequences(maxlen=max_sentence_len, sequences=x_train, padding=\"post\", value=n_voc - 1)\n",
    "\n",
    "y_train = [[tag2idx[w[1]] for w in s] for s in train_sentences]\n",
    "y_train = pad_sequences(maxlen=max_sentence_len, sequences=y_train, padding=\"post\", value=tag2idx['O'])\n",
    "\n",
    "x_dev = get_x_sequence(dev_sentences)\n",
    "x_dev = pad_sequences(maxlen=max_sentence_len, sequences=x_dev, padding=\"post\", value=n_voc - 1)\n",
    "\n",
    "y_dev = [[tag2idx[w[1]] for w in s] for s in dev_sentences]\n",
    "y_dev = pad_sequences(maxlen=max_sentence_len, sequences=y_dev, padding=\"post\", value=tag2idx['O'])\n",
    "\n",
    "x_test = get_x_sequence(test_sentences)\n",
    "x_test = pad_sequences(maxlen=max_sentence_len, sequences=x_test, padding=\"post\", value=n_voc - 1)\n",
    "\n",
    "y_test = [[tag2idx[w[1]] for w in s] for s in test_sentences]\n",
    "y_test = pad_sequences(maxlen=max_sentence_len, sequences=y_test, padding=\"post\", value=tag2idx['O'])\n",
    "\n",
    "y_train = [to_categorical(i, num_classes=n_tags) for i in y_train]\n",
    "y_dev = [to_categorical(i, num_classes=n_tags) for i in y_dev]\n",
    "y_test = [to_categorical(i, num_classes=n_tags) for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {}\n",
    "\n",
    "for tag in tag2idx.keys():\n",
    "  class_weights[tag2idx[tag]] = (1/train_tag.count(tag))*1000000\n",
    "  # class_weights[tag2idx[tag]] = len(train_text) / (n_tags * train_tag.count(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_size = max_sentence_len\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n_voc, \n",
    "                    output_dim=embedding_size, \n",
    "                    input_length=max_sentence_len,\n",
    "                    weights = [embedding_matrix], \n",
    "                    embeddings_initializer=k.initializers.Constant(embedding_matrix)))\n",
    "\n",
    "model.add(Bidirectional(LSTM(units=128, \n",
    "                           return_sequences=True, \n",
    "                           dropout=0.5, \n",
    "                           recurrent_dropout=0.5, \n",
    "                           kernel_initializer=k.initializers.he_normal())))\n",
    "\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "class F1ScoreCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, val_data, val_labels):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        self.validation_labels = val_labels\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        predicted_labels = self.model.predict(self.validation_data)\n",
    "        y_true = self.validation_labels\n",
    "\n",
    "        y_test_tags = []\n",
    "        y_pred_tags = []\n",
    "        for i in range(len(y_true)):\n",
    "            test_sent_tags = []\n",
    "            pred_sent_tags = []\n",
    "            for y, z in zip(y_true[i], predicted_labels[i]):\n",
    "                true_tag = list(tag2idx.keys())[list(tag2idx.values()).index(np.where(y == 1)[0][0])]\n",
    "                pred_tag = list(tag2idx.keys())[list(tag2idx.values()).index(np.argmax(z))]\n",
    "                test_sent_tags.append(true_tag)\n",
    "                pred_sent_tags.append(pred_tag)\n",
    "            y_test_tags.append(test_sent_tags)\n",
    "            y_pred_tags.append(pred_sent_tags)\n",
    "        \n",
    "        score = f1_score(y_test_tags, y_pred_tags)\n",
    "        logs['f1_score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1EarlyStopping(EarlyStopping):\n",
    "    def __init__(self, threshold=0.90, **kwargs):\n",
    "        super(F1EarlyStopping, self).__init__(**kwargs)\n",
    "        self.threshold = threshold  # The threshold for F1 score\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = self.get_monitor_value(logs)\n",
    "        if current is None:\n",
    "            return\n",
    "\n",
    "        # Only stop early if the F1 score is above the threshold\n",
    "        if current >= self.threshold: super().on_epoch_end(epoch, logs)\n",
    "        # If below the threshold, reset the number of no improvements\n",
    "        else: self.wait = 0\n",
    "\n",
    "    def get_monitor_value(self, logs):\n",
    "        logs = logs or {}\n",
    "        monitor_value = logs.get(self.monitor)\n",
    "        return monitor_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "batch_size = 64\n",
    "\n",
    "early_stopping = F1EarlyStopping(monitor='f1_score', \n",
    "                                 mode='max', \n",
    "                                 patience=10, \n",
    "                                 threshold=0.60, \n",
    "                                 verbose=1, \n",
    "                                 restore_best_weights = True)\n",
    "\n",
    "f1_metric = F1ScoreCallback(val_data=x_dev, val_labels=y_dev)\n",
    "\n",
    "# Save 'best' model\n",
    "model_path = '../Models/NER/'\n",
    "checkpoint = ModelCheckpoint(model_path + 'ner_model_f1_best.h5',  # Save the model to a file named 'model-<epoch_number>.h5'\n",
    "                             monitor='f1_score',      # Monitor validation loss\n",
    "                             verbose=1,               # Verbosity mode: 1 = print progress bar, 0 = silent\n",
    "                             save_best_only=True,     # Only save the model if 'val_loss' has improved\n",
    "                             mode='max')             # Mode: 'auto' decides whether to maximize or minimize 'val_loss' based on its name ('loss' should be minimized, 'acc' should be maximized)\n",
    "\n",
    "model.fit(x_train, np.array(y_train), batch_size=batch_size, epochs=num_epochs, shuffle=True, validation_data=(x_dev, np.array(y_dev)),\n",
    "         callbacks = [early_stopping, f1_metric, checkpoint], workers = 4, class_weight=class_weights)\n",
    "\n",
    "# Save latest model\n",
    "save_path = model_path + '/ner_model_last.h5'\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "load_path = '../Models/NER/ner_model.h5'\n",
    "loaded_model = load_model(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, np.array(y_test))\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tags = []\n",
    "y_pred_tags = []\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "  test_sent_tags = []\n",
    "  pred_sent_tags = []\n",
    "  for y, z in zip(y_test[i], predicted_labels[i]):\n",
    "    true_tag = list(tag2idx.keys())[list(tag2idx.values()).index(np.where(y == 1)[0][0])]\n",
    "    pred_tag = list(tag2idx.keys())[list(tag2idx.values()).index(np.argmax(z))]\n",
    "    test_sent_tags.append(true_tag)\n",
    "    pred_sent_tags.append(pred_tag)\n",
    "  y_test_tags.append(test_sent_tags)\n",
    "  y_pred_tags.append(pred_sent_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "f1_score(y_test_tags, y_pred_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_test)):\n",
    "  pred_tags = []\n",
    "  for x, y, z in zip(x_test[i], y_test[i], predicted_labels[i]):\n",
    "    word = list(word2idx.keys())[list(word2idx.values()).index(x)]\n",
    "    true_tag = list(tag2idx.keys())[list(tag2idx.values()).index(np.where(y == 1)[0][0])]\n",
    "    pred_tag = list(tag2idx.keys())[list(tag2idx.values()).index(np.argmax(z))]\n",
    "    print(word, true_tag, pred_tag)\n",
    "    pred_tags.append(pred_tag)\n",
    "  # print(pred_tags)\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
